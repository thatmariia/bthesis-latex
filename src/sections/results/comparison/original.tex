\subsubsection{Original data}

\paragraph{Statistical tests}

To compare the behavioral results to the simulation results, we first plotted the experimental data $\espres$ against simulation results $\simres$ data to see if there is a clear relationship between them (see Figure \ref{fig:relationship-original}). Looking at the plot, we noticed that the relationship between the two datasets seems to be monotonic with positive coefficients, meaning that the variables move in the same direction.

Next, we tested the relationship using the two-sided Pearson's correlation, which is a statistical test used to determine the strength and direction of the linear relationship between two continuous variables, with a confidence interval of 95\%. The correlation coefficient is a value in the range $[-1, 1]$ that indicates the strength of the relationship between the two variables: a coefficient of negative one indicates a strong negative relationship, zero indicates no relationship, and one indicates a strong positive relationship. The p-value, on the other hand, indicates the probability that the relationship between the two variables is due to chance. A low p-value (less than 0.05 in the case of the confidence interval of 95\%) indicates that the relationship is statistically significant and is unlikely to be due to chance.
The statistic we obtained was 0.6193, and the p-value was 0.0010. A correlation coefficient of this magnitude is typically thought to indicate a strong linear relationship.

However, we also ran a normality test on each dataset using the two-sided Kolmogorov-Smirnov test, a statistical test used to determine whether a sample comes from a normally distributed population, with a confidence level of 95\%. We found that the experimental data has a statistic of 0.6766 and a p-value of 7.406e-12, and the simulation data has a statistic of 0.7284 and a p-value of 4.842e-14, which indicates that neither dataset is normally distributed as the p-values are less than 0.05.

Because Pearson's correlation requires that both datasets be normally distributed for the results to be meaningful, we were unable to draw conclusions about the significance of the linear relationship between the experimental and simulation results data. We then turned to Spearman's correlation, which has no requirements for the distribution of data and can test the relationship of interest for monotonicity instead of linearity. 
We found that the test has a statistic of 0.5703 and a p-value of 0.0029, which allowed us to conclude that the relationship between the experimental and simulation results data is monotonic and positive. Similar to the obtained Pearson's statistic, a correlation coefficient of this magnitude is typically thought to indicate a strong monotonic positive relationship.

\paragraph{Regression analyses}

Although the original datasets are limited, we attempted to quantify the relationship between them further. We fit the polynomial curves of the first, second, and third degrees to the data using the LSGD method. The resulting polynomials are shown in Table \ref{tab:results-summary} and visualized in Figure \ref{fig:relationship-fit-original}. We evaluated the fit of the curves using two metrics: the R2 goodness-of-fit test and a 10-iteration Monte Carlo cross-validation (CV) with a 20-80\% split. 

The R2 value is a statistical measure that provides the proportion of the variance in the simulation data $\simres$ that can be predicted from the experimental data $\expres$. A value of R2 close to one indicates a good fit, while a value close to zero indicates a poor fit. In our case, the R2 values for the linear and quadratic curves were 0.3835 and 0.3871, respectively, typically indicating a poor or moderate fit. The cubic polynomial had an R2 of -4.1014, which is outside the range of $[0, 1]$, indicating that the model fits the data worse than the worst possible predictor, a horizontal line.

In addition to the R2 test, we also used a 10-iteration Monte Carlo CV with a 20-80\% split to evaluate the performance of the polynomial fits. In each iteration, we randomly shuffled the data and split it into a training set (80\% of the data) and a test set (20\% of the data). We then used the LSGD method to fit a polynomial to the training data and measured the mean squared error (MSE) on the test data. We repeated this process 10 times and averaged the MSE over all iterations. The resulting CV values for the polynomial fits of the first, second, and third degrees were 0.0023, 0.0045, and 0.0057, respectively. These values indicate that the fits performed well, as the CV values were small.

The summary of the statistical tests and the regression analyses performed on the original data can be found in Table \ref{tab:results-summary}.